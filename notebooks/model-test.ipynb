{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38664bittfgpu0edc226f184a4684901072b5de47f5d3",
   "display_name": "Python 3.8.6 64-bit ('tf-gpu')",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow.keras as tfk\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('../model/en_tokenizer')\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "oscar_corpus_tokenizer = Tokenizer.from_file('../model/oscar-corpus-tokenizer.json')\n",
    "oscar_corpus_tokenizer.get_vocab_size()"
   ]
  },
  {
   "source": [
    "### Load gpt2 model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained('../model/')\n",
    "len(model.transformer.h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.stop_gradient(model.transformer.get_input_embeddings().weight.value()).numpy()"
   ]
  },
  {
   "source": [
    "model parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[TensorShape([768]),\n",
       " TensorShape([768]),\n",
       " TensorShape([768, 2304]),\n",
       " TensorShape([1, 2304]),\n",
       " TensorShape([768, 768]),\n",
       " TensorShape([1, 768]),\n",
       " TensorShape([768]),\n",
       " TensorShape([768]),\n",
       " TensorShape([768, 3072]),\n",
       " TensorShape([1, 3072]),\n",
       " TensorShape([3072, 768]),\n",
       " TensorShape([1, 768])]"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "[i.shape for i in model.transformer.h[0].variables]"
   ]
  },
  {
   "source": [
    "## Create new embedding matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We need to get the mean embedding first as initial value for tokens that are not in the old vocab."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "mean_weights = tf.reduce_mean(weights, axis = 0).numpy()\n",
    "mean_weights.shape"
   ]
  },
  {
   "source": [
    "Create new embedding matrix with new vocab"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = oscar_corpus_tokenizer.get_vocab()\n",
    "old_vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2288, 376, 334, 32, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "enc = oscar_corpus_tokenizer.encode('hala mao ba?<|endoftext|>')\n",
    "enc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[20568, 276, 0], [10760, 1331, 21099, 35894, 276, 0]]"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "encoding = oscar_corpus_tokenizer.encode_batch(['naunsa ka<|endoftext|>', 'hello nimo dodong kahibaw ka<|endoftext|>'])\n",
    "foo = [i.ids for i in  encoding]\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "new_embeddings = tf.zeros([len(new_vocab), mean_weights.shape[0]]).numpy()\n",
    "new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/50257 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d55256fbe332487a8af61f487e7108a4"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "in_en_vocab = []\n",
    "not_in_en_vocab = []\n",
    "for word, idx_new  in tqdm(new_vocab.items()):\n",
    "    idx_old =  old_vocab.get(word, -1)\n",
    "\n",
    "    if idx_old >= 0:\n",
    "        new_embeddings[idx_new, :] = weights[idx_old, :]\n",
    "        in_en_vocab.append(word)\n",
    "    else:\n",
    "        new_embeddings[idx_new, :] = mean_weights\n",
    "        not_in_en_vocab.append(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10244\n40013\n"
     ]
    }
   ],
   "source": [
    "print(len(in_en_vocab))\n",
    "print(len(not_in_en_vocab))"
   ]
  },
  {
   "source": [
    "set new embeddings "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.set_input_embeddings(tf.constant(new_embeddings))"
   ]
  },
  {
   "source": [
    "test out model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['kamusta naman ka?<|endoftext|>', 'maayong buntag!<|endoftext|>']\n",
    "oscar_corpus_tokenizer.enable_padding()\n",
    "encoded = oscar_corpus_tokenizer.encode_batch(inputs)\n",
    "inputs = np.array([i.ids for i in encoded])\n",
    "#print(inputs)\n",
    "\n",
    "#result = model(inputs)\n",
    "#print(result.logits)"
   ]
  },
  {
   "source": [
    "### Freezing weights for fine tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscar_corpus_tokenizer.enable_truncation()"
   ]
  },
  {
   "source": [
    "For this implementation, only freeze the inner layers. Do not freeze layer norm, wte, and wpe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "def freeze_weights_vanilla(model):\n",
    "    for layer in model.transformer.h:\n",
    "        layer.trainable = False\n",
    "    model.transformer.wte.trainable = True\n",
    "    model.transformer.wpe.trainable = True\n",
    "    model.transformer.ln_f.trainable = True\n",
    "    \n",
    "# check if all are frozen\n",
    "freeze_weights_vanilla(model)\n",
    "all([not l.trainable for l in model.transformer.h])\n"
   ]
  },
  {
   "source": [
    "### train-test split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c555cd56f6c41a4ac2803ba5166d661"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            document  \\\n",
       "0  matamwa lg ko mayung gabe sa tanan....balikbay...   \n",
       "1  Maayong kaagahon liwat diri sa payag ICCC...na...   \n",
       "2  tani ari si pre idol toto jericp SR, para siya...   \n",
       "3  Kapital sa munisipyo ang Āmol (Pinulongang Per...   \n",
       "4  ↑ Kalkulado gikan sa pakigbingkil sa tanan nga...   \n",
       "\n",
       "                                       truncated_doc  \n",
       "0  matamwa lg ko mayung gabe sa tanan....balikbay...  \n",
       "1  Maayong kaagahon liwat diri sa payag ICCC...na...  \n",
       "2  tani ari si pre idol toto jericp SR, para siya...  \n",
       "3  Kapital sa munisipyo ang Āmol (Pinulongang Per...  \n",
       "4  ↑ Kalkulado gikan sa pakigbingkil sa tanan nga...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>truncated_doc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>matamwa lg ko mayung gabe sa tanan....balikbay...</td>\n      <td>matamwa lg ko mayung gabe sa tanan....balikbay...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Maayong kaagahon liwat diri sa payag ICCC...na...</td>\n      <td>Maayong kaagahon liwat diri sa payag ICCC...na...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tani ari si pre idol toto jericp SR, para siya...</td>\n      <td>tani ari si pre idol toto jericp SR, para siya...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Kapital sa munisipyo ang Āmol (Pinulongang Per...</td>\n      <td>Kapital sa munisipyo ang Āmol (Pinulongang Per...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>↑ Kalkulado gikan sa pakigbingkil sa tanan nga...</td>\n      <td>↑ Kalkulado gikan sa pakigbingkil sa tanan nga...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "text_lines = []\n",
    "ptt = re.compile(r'\\n$|\\[\\d+\\]')\n",
    "with open('../shuff-dedup/ceb/ceb_dedup.txt', 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        clean_text = ptt.sub('', line)\n",
    "        text_lines.append(clean_text)\n",
    "df_txt = pd.DataFrame({'document': text_lines})\n",
    "\n",
    "# add end of line token\n",
    "df_txt['truncated_doc'] = df_txt.document.apply(lambda x: x + '<|endoftext|>')\n",
    "df_txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscar_corpus_tokenizer.enable_truncation(max_length=1024)\n",
    "def PreprocessData(ids):\n",
    "    docs = df_txt.iloc[ids,:].document \\\n",
    "        .map(lambda str: str + '<|endoftext|>').tolist()\n",
    "    pad_token_id = oscar_corpus_tokenizer.token_to_id('<pad>')\n",
    "    input = []\n",
    "    labels = []\n",
    "    attn_mask = []\n",
    "    for doc in docs:\n",
    "        encoded = oscar_corpus_tokenizer.encode(doc)\n",
    "        input.append(encoded.ids[:-1])\n",
    "        labels.append(encoded.ids[1:])\n",
    "        attn_mask.append(encoded.attention_mask[:-1])\n",
    "        \n",
    "    input = pad_sequences(input, value = pad_token_id, padding='post')\n",
    "    labels = pad_sequences(labels, value = pad_token_id, padding='post')\n",
    "    attn_mask = pad_sequences(attn_mask, value = 0, padding='post')\n",
    "\n",
    "    #return one hot tensor\n",
    "    #labels = tf.one_hot(labels, depth=new_embeddings.shape[0], dtype=tf.int32)\n",
    "\n",
    "    return input, labels, attn_mask\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = len(text_lines)\n",
    "train_split = 0.8\n",
    "train_num_docs = int(num_lines * train_split)\n",
    "train_ids = np.random.choice(num_lines, train_num_docs, replace = False)\n",
    "test_ids = np.setdiff1d(np.arange(num_lines), train_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60176\n15044\n"
     ]
    }
   ],
   "source": [
    "print(train_ids.shape[0])\n",
    "print(test_ids.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"tst_df = Dataset.from_tensor_slices(train_ids) \\\n",
    "    .batch(1) \\\n",
    "    .map(lambda x: tf.py_function(PreprocessData, [x], [tf.int32, tf.int32, tf.int32]))\n",
    "\n",
    "length = []\n",
    "for inp, _, __ in tqdm(tst_df):\n",
    "    length.append(inp.shape[1])\n",
    "pd.Series(length).describe()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 1\n",
    "df_train = Dataset.from_tensor_slices(train_ids)\n",
    "df_train = df_train.shuffle(10000).batch(train_batch_size)\n",
    "df_train = df_train.map(lambda x: tf.py_function(PreprocessData, [x], [tf.int32, tf.int32, tf.int32]))"
   ]
  },
  {
   "source": [
    "## Training routine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true,y_pred):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y_true, y_pred))\n"
   ]
  },
  {
   "source": [
    "epochs = 1\n",
    "lr = int(1e-4)\n",
    "num_batches = train_ids.shape[0] // train_batch_size\n",
    "optimizer = tfk.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "for i in range(epochs):\n",
    "    for inp, label, attn  in tqdm(df_train.take(10)):\n",
    "        batch_losses = []\n",
    "        with tf.GradientTape() as tape:\n",
    "            results = model(input_ids = inp, attention_mask = label)\n",
    "            loss = cross_entropy_loss(label, results.logits)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "             "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}