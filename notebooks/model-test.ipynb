{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('../model/en_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscar_corpus_tokenizer = Tokenizer.from_file('../model/oscar-corpus-tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "oscar_corpus_tokenizer.get_vocab_size()"
   ]
  },
  {
   "source": [
    "### Load gpt2 model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained('../model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(model.transformer.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.stop_gradient(model.transformer.get_input_embeddings().weight.value()).numpy()"
   ]
  },
  {
   "source": [
    "## Create new embedding matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We need to get the mean embedding first as initial value for tokens that are not in the old vocab."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "mean_weights = tf.reduce_mean(weights, axis = 0).numpy()\n",
    "mean_weights.shape"
   ]
  },
  {
   "source": [
    "Create new embedding matrix with new vocab"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = oscar_corpus_tokenizer.get_vocab()\n",
    "old_vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "new_embeddings = tf.zeros([len(new_vocab), mean_weights.shape[0]]).numpy()\n",
    "new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8331515d7bd048b4a47015f013319dfa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "in_en_vocab = []\n",
    "not_in_en_vocab = []\n",
    "for word, idx_new  in tqdm(new_vocab.items()):\n",
    "    idx_old =  old_vocab.get(word, -1)\n",
    "\n",
    "    if idx_old >= 0:\n",
    "        new_embeddings[idx_new, :] = weights[idx_old, :]\n",
    "        in_en_vocab.append(word)\n",
    "    else:\n",
    "        new_embeddings[idx_new, :] = mean_weights\n",
    "        not_in_en_vocab.append(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7198\n17802\n"
     ]
    }
   ],
   "source": [
    "print(len(in_en_vocab))\n",
    "print(len(not_in_en_vocab))"
   ]
  },
  {
   "source": [
    "set new embeddings "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.set_input_embeddings(tf.constant(new_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscar_corpus_tokenizer.enable_padding()\n",
    "inputs = oscar_corpus_tokenizer.encode_batch(['naunsa naman ka diha?', 'okay ra ka?'])\n",
    "inputs = np.array([i.ids for i in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(inputs)"
   ]
  },
  {
   "source": [
    "test predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       "array([[   11, 22995,    12,   361,   198],\n",
       "       [   11,   325,    30,   198,   198]])>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "foo = tf.nn.softmax(logits=result.logits, axis = 2)\n",
    "tf.argmax(foo, axis = 2)"
   ]
  },
  {
   "source": [
    "### Freezing weights for fine tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For this implementation, only freeze the inner layers. Do not freeze layer norm, wte, and wpe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "def freeze_weights_vanilla(model):\n",
    "    for layer in model.transformer.h:\n",
    "        layer.trainable = False\n",
    "    model.transformer.wte.trainable = True\n",
    "    model.transformer.wpe.trainable = True\n",
    "    model.transformer.ln_f.trainable = True\n",
    "    \n",
    "# check if all are frozen\n",
    "freeze_weights_vanilla(model)\n",
    "all([not l.trainable for l in model.transformer.h])\n"
   ]
  },
  {
   "source": [
    "### train-test split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = sum(1 for line in open('../shuff-dedup/ceb/ceb_dedup.txt'))\n",
    "train_split = 0.8\n",
    "train_num_docs = int(num_lines * train_split)\n",
    "train_ids = np.random.choice(num_lines, train_num_docs, replace = False)\n",
    "test_ids = np.setdiff1d(np.arange(num_lines), train_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60176\n15044\n"
     ]
    }
   ],
   "source": [
    "print(train_ids.shape[0])\n",
    "print(test_ids.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[TensorShape([768]),\n",
       " TensorShape([768]),\n",
       " TensorShape([768, 2304]),\n",
       " TensorShape([1, 2304]),\n",
       " TensorShape([768, 768]),\n",
       " TensorShape([1, 768]),\n",
       " TensorShape([768]),\n",
       " TensorShape([768]),\n",
       " TensorShape([768, 3072]),\n",
       " TensorShape([1, 3072]),\n",
       " TensorShape([3072, 768]),\n",
       " TensorShape([1, 768])]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "[i.shape for i in model.transformer.h[0].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}